{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks, TensorFlow, and its Estimators Interface (with an eye towards learning quantifiers)\n",
    "\n",
    "### About this notebook:\n",
    "This notebook was written by Shane Steinert-Threlkeld for the Neural Network Methods for Quantifiers coordinated project at the ILLC, Universiteit van Amsterdam in January 2018 (http://shane.st/NNQ).  \n",
    "\n",
    "It introduces the basics of working with TensorFlow to train neural networks, with an eye to applications to quantifiers.  In particular, the code is a warm-up to understanding this repository: https://github.com/shanest/quantifier-rnn-learning. The main components of that code that this notebook doesn't directly cover are the `lstm_model_fn` and the data generation process (`data_gen.py`), though simpler analogues of both are here.\n",
    "\n",
    "There are three sections:\n",
    "\n",
    "1. Basic TF abstractions: sessions, the graph, Variables/Placeholders\n",
    "2. Training a feed-forward neural network to classify bit sequences\n",
    "3. Re-doing the above using TF estimators  \n",
    "\n",
    "#### Intended working environment for this notebook:\n",
    "* Python 2.7\n",
    "* Tensorflow 1.4\n",
    "\n",
    "To run: (i) install Jupyter; (ii) save this .ipynb file in a directory; (iii) from that directory, run `jupyter notebook`; (iv) open this file.\n",
    "\n",
    "### License\n",
    "Copyright 2018 Shane Steinert-Threlkeld\n",
    "\n",
    "> This program is free software: you can redistribute it and/or modify\n",
    "> it under the terms of the GNU General Public License as published by\n",
    "> the Free Software Foundation, either version 3 of the License, or\n",
    "> (at your option) any later version.\n",
    ">\n",
    "> This program is distributed in the hope that it will be useful,\n",
    "> but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "> MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "> GNU General Public License for more details.\n",
    ">\n",
    "> You should have received a copy of the GNU General Public License\n",
    "> along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TensorFlow Mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and running a computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = tf.constant(3.0)\n",
    "c2 = tf.constant(4.0)\n",
    "print c1\n",
    "\n",
    "add1 = tf.add(c1, c2)\n",
    "add2 = c1 + c2 #same as above, though I prefer to use the `tf.` versions of ops, to be most clear\n",
    "print add1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that what's printed is not the value 3.0, but a Tensor, a TF data-type corresponding to a node in the computational graph.\n",
    "\n",
    "To get its value, we need to _run_ the graph inside a _session_.\n",
    "\n",
    "[Note: it's always good to use a `with` block to wrap a session, so that it closes automatically.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print sess.run(c1)\n",
    "    print sess.run(add1)\n",
    "    # you can also pass a list of ops instead of a single op to `run`\n",
    "    print sess.run([c1, c2, add1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors also have a _shape_, telling you what how many dimensions, and the size of each dimension.  I find it to be a good practice to include the shape as a comment above every operation.  Because the shape is a property of the `Tensor`, it can be accessed without running the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- mat: [3, 2]\n",
    "mat = tf.constant([[1.0, 2.0],\n",
    "                   [3.0, 4.0],\n",
    "                   [5.0, 6.0]])\n",
    "print mat.shape\n",
    "\n",
    "# -- vec: [2, 1]\n",
    "vec = tf.constant([[1.0],\n",
    "                   [1.0]])\n",
    "\n",
    "# -- mul: [3, 1]\n",
    "mul = tf.matmul(mat, vec)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and placeholders\n",
    "\n",
    "A neural network learns to approximate a given function by seeing exmples and updating its _parameters_ in order to do a better job at approximating the data it has seen.  While we fore-stall an actual discussion of training to the next section, we note two other pieces of machinery that are required for this:\n",
    "\n",
    "1. Variables: these are `Tensor`s whose values can be changed.  So parameters of a model -- and anything else you want to be updated -- will be Variables.\n",
    "2. Placeholders: these are `Tensor`s that represent input to the network/computational graph: their value must be provided externally via what TensorFlow calls a `feed_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable([[1.0, 2.0],\n",
    "                   [3.0, 4.0],\n",
    "                   [5.0, 6.0]])\n",
    "b = tf.Variable([[1.0],\n",
    "                 [1.0], \n",
    "                 [1.0]])\n",
    "\n",
    "x = tf.placeholder(shape=(2,1), dtype=tf.float32)\n",
    "\n",
    "linear = tf.matmul(W, x)\n",
    "result = tf.add(linear, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # variables must be initialized\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # result depends on a placeholder, so input must be fed in\n",
    "    print sess.run(result, feed_dict={x: [[1.0], [1.0]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the shape of the placeholder `x` was specified precisely.  While this is good practice, it's often convenient to leave one of the dimensions as `None`, so that batches of different numbers of input can be sent to the model.  (For example, mini-batches during training, one big batch during evaluation.  We'll see how this works later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a feed-forward neural network to learn 'at least three'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating labeled data\n",
    "\n",
    "First, we will generate labeled data.  \n",
    "\n",
    "The Xs will be all sequences of 0s and 1s of a specified length.\n",
    "\n",
    "The Ys will be labels -- 0 or 1 -- provided by a user-defined function that takes a sequence as its input.  Here we provide one: `at_least_three`.\n",
    "\n",
    "The data is shuffled, so that the order is random.  Finally, it is split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools as iter\n",
    "import random\n",
    "import math\n",
    "\n",
    "def generate_all_seqs(length, shuffle=True):\n",
    "    seqs = list(iter.product([0,1], repeat=length))\n",
    "    if shuffle:\n",
    "        random.shuffle(seqs)\n",
    "    return seqs\n",
    "\n",
    "def at_least_three(seq):\n",
    "    # we return [0,1] for True and [1,0] for False\n",
    "    return [0,1] if sum(seq) >= 3 else [1,0]\n",
    "\n",
    "def get_labeled_data(seqs, func):\n",
    "    return seqs, [func(seq) for seq in seqs]\n",
    "\n",
    "# generate all labeled data\n",
    "SEQ_LEN = 16\n",
    "NUM_CLASSES = 2\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "X, Y = get_labeled_data(generate_all_seqs(SEQ_LEN), at_least_three)\n",
    "\n",
    "# split into training and test sets\n",
    "pivot_index = int(math.ceil(TRAIN_SPLIT*len(X)))\n",
    "\n",
    "trainX, trainY = X[:pivot_index], Y[:pivot_index]\n",
    "testX, testY = X[pivot_index:], Y[pivot_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network to classify sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the neural network inside a wrapper class which helps readability, separation of code components (graph building, session management/training, et cetera), and the ability to test many different models on the same data.\n",
    "\n",
    "The initializer builds a simple feed-forward neural network with one hidden layer.\n",
    "\n",
    "Instances of the class have properties for training, predicting, and evaluating, as well as for inputting sequences and labels.  These are the corresponding ops in the graph, so they can be passed directly to `Session.run()` and used in `feed_dict`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FFNN(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size=10):\n",
    "        \n",
    "        # first, basic network architecture\n",
    "        \n",
    "        # -- inputs: [batch_size, input_size]\n",
    "        inputs = tf.placeholder(shape=[None, input_size], dtype=tf.float32)\n",
    "        self._inputs = inputs\n",
    "        # -- labels: [batch_size, output_size]\n",
    "        labels = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
    "        self._labels = labels\n",
    "        \n",
    "        # we will have one hidden layer\n",
    "        # in general, this should be parameterized\n",
    "        \n",
    "        # -- weights1: [input_size, hidden_size]\n",
    "        weights1 = tf.Variable(tf.random_uniform(shape=[input_size, hidden_size]))\n",
    "        # -- biases1: [hidden_size]\n",
    "        biases1 = tf.Variable(tf.random_uniform(shape=[hidden_size]))\n",
    "        # -- linear: [batch_size, hidden_size]\n",
    "        linear = tf.add(tf.matmul(inputs, weights1), biases1)\n",
    "        # -- hidden: [batch_size, hidden_size]\n",
    "        hidden = tf.nn.relu(linear)\n",
    "        \n",
    "        # -- weights2: [hidden_size, output_size]\n",
    "        weights2 = tf.Variable(tf.random_uniform(shape=[hidden_size, output_size]))\n",
    "        # -- biases2: [output_size]\n",
    "        biases2 = tf.Variable(tf.random_uniform(shape=[output_size]))\n",
    "        # -- logits: [batch_size, output_size]\n",
    "        logits = tf.add(tf.matmul(hidden, weights2), biases2)\n",
    "        \n",
    "        # second, define loss and training\n",
    "        # -- cross_entropy: [batch_size]\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=labels,\n",
    "                logits=logits)\n",
    "        # -- loss: []\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self._train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        # finally, some evaluation ops\n",
    "        \n",
    "        # -- probabilities: [batch_size, output_size]\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        self._probabilities = probabilities\n",
    "        # -- predictions: [batch_size]\n",
    "        predictions = tf.argmax(probabilities, axis=1)\n",
    "        # -- targets: [batch_size]\n",
    "        targets = tf.argmax(labels, axis=1)\n",
    "        # -- correct_prediction: [batch_size]\n",
    "        correct_prediction = tf.equal(predictions, targets)\n",
    "        # -- accuracy: []\n",
    "        accuracy = tf.reduce_mean(tf.to_float(correct_prediction))\n",
    "        # more evaluation ops could be added here\n",
    "        self._eval_dict = {\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train_op\n",
    "    \n",
    "    @property\n",
    "    def predictions(self):\n",
    "        return self._probabilities\n",
    "    \n",
    "    @property\n",
    "    def evaluate(self):\n",
    "        return self._eval_dict\n",
    "    \n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "    \n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Here we see the main training loop for our neural network.  There are two key parameters to training:\n",
    "* number of epochs: how many times to iterate through the whole training set\n",
    "* batch size: how large each mini-batch should be.  In other words, the network will receive this many labeled examples before computing loss and gradients and updating its parameters.\n",
    "\n",
    "In general, mini-batches of medium size strike a good balance between speed and variance.  If batch size is the size of the training set, then there's no variance in the estimate of the loss and gradients; if the batch size is 1, there's a tremendous amount of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the graph before building a model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # build our model\n",
    "    model = FFNN(SEQ_LEN, NUM_CLASSES)\n",
    "    # initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # MAIN TRAINING LOOP\n",
    "    NUM_EPOCHS = 2\n",
    "    BATCH_SIZE = 12\n",
    "    num_batches = len(trainX) / BATCH_SIZE\n",
    "    \n",
    "    for epoch in xrange(NUM_EPOCHS):\n",
    "        \n",
    "        # shuffle the training data at start of each epoch\n",
    "        train_data = zip(trainX, trainY)\n",
    "        random.shuffle(train_data)\n",
    "        trainX = [datum[0] for datum in train_data]\n",
    "        trainY = [datum[1] for datum in train_data]\n",
    "        \n",
    "        for batch_idx in xrange(num_batches):\n",
    "            # get batch of training data\n",
    "            batchX = trainX[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
    "            batchY = trainY[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
    "            # train on the batch\n",
    "            sess.run(model.train, \n",
    "                     {model.inputs: batchX,\n",
    "                      model.labels: batchY})\n",
    "            \n",
    "            # evaluate every N training steps (batches)\n",
    "            if batch_idx % 50 == 0:\n",
    "                print '\\nEpoch {}, batch {}, evaluation'.format(epoch, batch_idx)\n",
    "                print sess.run(model.evaluate, {model.inputs: testX, model.labels: testY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Re-writing the above using TensorFlow Estimator\n",
    "\n",
    "The TensorFlow Estimator API -- https://www.tensorflow.org/api_docs/python/tf/estimator -- provides convenience functions that handle a lot of the nitty-gritty around running a training loop, feeding in input data, and things of that sort.\n",
    "\n",
    "Another benefit of the API: _it automatically saves and loads trained models for you_, if you use the `model_dir` argument.\n",
    "\n",
    "First, we use the library's pre-built `DNNClassifier` estimator, to show the mechanics of training and evaluating.  The basic thing to note is that we have to wrap our training and test datasets in `input_function`s, so that TensorFlow knows how to feed them to the estimator.\n",
    "\n",
    "In the next section, we will convert our `FFNN` class above into a custom-built `estimator`, to see in more detail how the API works.  This is especially important since there are not yet pre-made estimators for RNNs, so the code at https://github.com/shanest/quantifier-rnn-learning implements a custom estimator.\n",
    "\n",
    "In that next section, I will also show how to implement evaluation inside of a training loop, instead of waiting until the end of training.  This is important for our kind of experiments, which want to measure performance on the test set as training proceeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[SEQ_LEN])]\n",
    "\n",
    "# The library has a pre-made DNNClassifier class\n",
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                       hidden_units=[10],\n",
    "                                       n_classes=NUM_CLASSES,\n",
    "                                       optimizer=tf.train.AdamOptimizer())\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(trainX)},\n",
    "    # DNNClassifier wants integer labels, so take argmax of e.g. [0,1] here\n",
    "    y=np.argmax(trainY, axis=1),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=1,\n",
    "    shuffle=True)\n",
    "\n",
    "classifier.train(input_fn=train_input_fn)\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(testX)},\n",
    "    # DNNClassifier wants integer labels, so take argmax of e.g. [0,1] here\n",
    "    y=np.argmax(testY, axis=1),\n",
    "    # one big batch, instead of mini-batches\n",
    "    batch_size=len(testX),\n",
    "    shuffle=False)\n",
    "\n",
    "classifier.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a custom estimator via a model_fn\n",
    "\n",
    "To use the `tf.estimator` library with your own models, one has to define a `model_fn`.  In this section, we convert the above `FFNN.__init__` method into such a function. I will also use `tf.layers` to simplify the code. \n",
    "\n",
    "Doing so allows one to reap the benefits of `estimator` while using novel models and/or models for which TF hasn't implemented pre-built estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# required arguments; params will contain anything custom you want to pass to the model-building function\n",
    "def ffnn_model_fn(features, labels, mode, params):\n",
    "    \n",
    "    # basic network \n",
    "    \n",
    "    # -- inputs: [batch_size, input_size]\n",
    "    inputs = tf.to_float(features[\"x\"])\n",
    "    # -- hidden: [batch_size, hidden_size]\n",
    "    hidden = tf.layers.dense(inputs, params['hidden_size'],\n",
    "                            activation=params['hidden_activation'])\n",
    "    # -- logits: [batch_size, num_classes]\n",
    "    # note: default for tf.layers.dense is no activation, i.e. linear\n",
    "    logits = tf.layers.dense(hidden, params['num_classes'])\n",
    "\n",
    "    # predictions\n",
    "    # -- probs: [batch_size, num_classes]\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    # predictions to be output; can be customized!\n",
    "    out_preds = {'probs': probs,\n",
    "                'hidden': hidden}\n",
    "    \n",
    "    # NOTE: prediction mode needs to be handled first, since TF\n",
    "    # automatically passes `None` for the `labels` argument in this\n",
    "    # mode and most loss functions throw an error in that situation\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                         predictions=out_preds)\n",
    "    \n",
    "    # training\n",
    "    # -- cross_entropy: [batch_size]\n",
    "    cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=labels,\n",
    "        logits=logits)\n",
    "    # -- loss: []\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    # it's important to pass global_step here!\n",
    "    train_op = optimizer.minimize(loss,\n",
    "                                 global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # evaluation metrics\n",
    "    \n",
    "    # -- predictions: [batch_size]\n",
    "    predictions = tf.argmax(probs, axis=1)\n",
    "    # -- targets: [batch_size]\n",
    "    targets = tf.argmax(labels, axis=1)\n",
    "    # -- accuracy: scalar\n",
    "    accuracy = tf.metrics.accuracy(targets, predictions)\n",
    "    \n",
    "    # evaluation metrics to be output; can be customized!\n",
    "    eval_metrics = {'accuracy': accuracy}\n",
    "    \n",
    "    # return an estimator spec, specifying mode, loss, train op, predictions, and evaluation metrics\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                     loss=loss,\n",
    "                                     train_op=train_op,\n",
    "                                     eval_metric_ops=eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have built our `model_fn`, we can train in much the same way as before.  The only real difference is that we pass our new function as the `model_fn` argument to `tf.estimator.Estimator`, instead of initializing one of the predefined estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# hyperparameters\n",
    "hparams = {'hidden_size': 10, 'hidden_activation': tf.nn.relu, 'num_classes': 2}\n",
    "\n",
    "# to build custom estimator, use model_fn and params arguments\n",
    "estimator = tf.estimator.Estimator(model_fn=ffnn_model_fn, params=hparams)\n",
    "\n",
    "new_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(trainX)},\n",
    "    y=np.array(trainY),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True)\n",
    "\n",
    "estimator.train(input_fn=new_train_input_fn, steps=50)\n",
    "\n",
    "new_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(testX)},\n",
    "    y=np.array(testY),\n",
    "    batch_size=len(testX),\n",
    "    shuffle=False)\n",
    "\n",
    "estimator.evaluate(input_fn=new_test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction: getting output from an `Estimator`\n",
    "\n",
    "The `predict` method allows you to generate predictions from given inputs to the model.  The intended use for this is to use a trained model to generate predictions on new data.  For example, a trained image classifier can be given a new image to label.\n",
    "\n",
    "For the purposes of this project, however, we can make use of some flexibility in `predict`: in your `model_fn`, you get to specify exactly what the `predict` method outputs, as a dictionary whose keys are names and values are `Tensor`s (which will be evaluated).\n",
    "\n",
    "**EXERCISE**: modify `ffnn_model_fn` so that the hidden layer is also output by `predict`.  For what might this be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(testX[:5])},\n",
    "    shuffle=False)\n",
    "\n",
    "predictions = list(estimator.predict(input_fn=predict_input_fn))\n",
    "print predictions\n",
    "for idx in range(5):\n",
    "    print '{}: {}'.format(testX[idx], predictions[idx]['probs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping and continuous evaluation using SessionRunHook\n",
    "\n",
    "Using `tf.estimator.Estimator.train`, while convenient in many ways, appears to give us less control over the training loop.  When we manually managed training, it was easy to evaluate during training and to do early stopping (i.e. stop training when a certain condition is met, instead of when the entire training cycle is over).  \n",
    "\n",
    "Luckily, we can re-create these abilities using `SessionRunHook`.  While there are still disadvantages (the model has to be saved/loaded everytime you want to evaluate), the net benefits of `estimator` are positive.\n",
    "\n",
    "With a `SessionRunHook`, you implement behavior that you want to execute before and after every `session.run` call made by `estimator.train()`.  Full documentation here: https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook.\n",
    "\n",
    "To implement evaluation during training, we make a new class, extending `SessionRunHook`, which takes a given estimator and input_function, and evaluates the estimator on that input every N steps.  The more complicated Hook in the main repository also implements early stopping and writes the data to a CSV file at the end of training.\n",
    "\n",
    "**EXERCISE**: implement an early stopping condition in `after_run` of the hook below.  Example: when the evaluation loss is below a certain threshold. Hint: `run_context.request_stop()` should be called when your stop condition is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDuringHook(tf.train.SessionRunHook):\n",
    "    \n",
    "    def __init__(self, estimator, input_fn, num_steps=50):\n",
    "        self._estimator = estimator\n",
    "        self._input_fn = input_fn\n",
    "        self._num_steps = num_steps\n",
    "        \n",
    "    def begin(self):\n",
    "\n",
    "        # get the tensor that keeps track of the global step\n",
    "        self._global_step_tensor = tf.train.get_or_create_global_step()\n",
    "        if self._global_step_tensor is None:\n",
    "            raise ValueError(\"global_step needed for EvalEarlyStop\")\n",
    "\n",
    "    # before session run calls, put here the tensors you want to run\n",
    "    # these will be given to after_run\n",
    "    def before_run(self, run_context):\n",
    "\n",
    "        requests = {'global_step': self._global_step_tensor}\n",
    "        return tf.train.SessionRunArgs(requests)\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "\n",
    "        global_step = run_values.results['global_step']\n",
    "        # evaluate and print if it's the right number of steps\n",
    "        if (global_step-1) % self._num_steps == 0:\n",
    "            eval_results = self._estimator.evaluate(input_fn=self._input_fn)\n",
    "            print eval_results\n",
    "            if eval_results['loss'] < 0.05:\n",
    "                run_context.request_stop()\n",
    "            \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# now, we train, passing the Hook to the method\n",
    "# it's useful to tell TF to also save checkpoints every N steps, which we do with a custom RunConfig\n",
    "num_steps = 50\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps=num_steps,\n",
    "        save_checkpoints_secs=None)\n",
    "\n",
    "new_estimator = tf.estimator.Estimator(model_fn=ffnn_model_fn, params=hparams, config=run_config)\n",
    "\n",
    "new_estimator.train(input_fn=new_train_input_fn,\n",
    "                hooks=[EvalDuringHook(new_estimator, new_test_input_fn)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
